# -*- coding: utf-8 -*-
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Literal, Optional

import h5py
import numpy as np


def _find_val(val, grp, prefix=""):
    """Find value in hdf5 groups"""
    for obj_name, obj in grp.items():
        if obj_name in ("SPICElogs", "data"):
            continue
        else:
            path = f"{prefix}/{obj_name}"
            if val == obj_name:
                return path
            # test for group (go down)
            elif isinstance(obj, h5py.Group):
                gpath = _find_val(val, obj, path)
                if gpath:
                    return gpath


def _read_spice(file_name):
    """Reads an ascii generated by spice, and returns a header structure and a data table

    Args:
        file_name (str): a string containing the filename

    Returns:
        spice_data (np.ndarray): an array containing all columns/rows
        headers (dict): a dictionary containing information from the commented lines.
        col_headers (tuple): name of each collum in spice_data
        unused (tuple): not yet used in hdf5
    """
    with open(file_name, encoding="utf-8") as f:
        all_content = f.readlines()

    metadata = [line.strip() for line in all_content if "#" in line]
    index_col_name = metadata.index("# col_headers =")
    col_headers = tuple(metadata[index_col_name + 1].strip("#").split())
    metadata_list = metadata[:index_col_name]
    error_messages = metadata[index_col_name + 2 :]

    index_sum_count = [i for i, header in enumerate(metadata) if header.startswith("# Sum of Counts =")]
    # in case "Sum of Counts" doesn't exist
    # happens to the last scan after beam is down
    if len(index_sum_count) != 0:
        metadata_list += metadata[index_sum_count[0] :]
        error_messages = tuple(error_messages[: index_sum_count[0] - len(metadata)])

    headers = {}
    unused = []

    for metadata_entry in metadata_list:
        line = metadata_entry.strip("# ")

        if "completed" in line or "stopped" in line:  # last line
            parts = line.split(" ")
            headers["end_time"] = parts[3] + " " + parts[0] + " " + parts[1]
        # elif line[-1] == "=":  # empty line
        #     unused.append(line[:-2])  # remove  " ="
        elif "=" in line:  # useful line
            parts = line.split("=")
            key = parts[0].strip()
            val = "=".join(parts[1:])[1:]  # remove the fisrt space charactor
            headers[key] = val
        else:  # how did you get here?
            unused.append(line)
    unused = tuple(unused)

    spice_data = np.genfromtxt(file_name, comments="#")

    return spice_data, col_headers, headers, unused, error_messages


def _read_spice_ub(ub_file_name):
    """Reads ub info from UBConf"""
    ubconf = {}
    with open(ub_file_name, encoding="utf-8") as f:
        all_content = f.readlines()

    for idx, line in enumerate(all_content):
        if line.strip() == "":
            continue  # skip if empty
        elif line.strip()[0] == "[":
            continue  # skiplines like "[xx]"

        ub_dict = {}
        key, val = line.strip().split("=")
        if key == "Mode":
            if all_content[idx - 1].strip() == "[UBMode]":
                ub_dict["UBMode"] = int(val)
            elif all_content[idx - 1].strip() == "[AngleMode]":
                ub_dict["AngleMode"] = int(val)
        else:
            if "," in line:  # vector
                ub_dict[key] = np.array([float(v) for v in val.strip('"').split(",")])
            else:  # float number
                ub_dict[key] = float(val)

        ubconf.update(ub_dict)

    return ubconf


def _format_spice_header(headers):

    formatted_headers = {}

    exp_str = ["scan_title", "users", "local_contact", "experiment"]
    for k, v in headers.items():
        if "," in v and k not in exp_str:  # vectors
            vec = np.array([float(v0) for v0 in v.split(",")])
            formatted_headers.update({k: vec})
        elif v.replace(".", "").isnumeric():  # numebrs only
            if v.isdigit():  # int
                formatted_headers.update({k: int(v)})
            else:  # float
                formatted_headers.update({k: float(v)})
        # separate COM/FWHM and its errorbar
        elif k == "Center of Mass":
            if v == "NaN+/-NaN":
                formatted_headers.update({"COM": np.nan, "COM_err": np.nan})
            else:
                com, e_com = v.split("+/-")
                formatted_headers.update({"COM": float(com), "COM_err": float(e_com)})
        elif k == "Full Width Half-Maximum":
            if v == "NaN+/-NaN":
                formatted_headers.update({"FWHM": np.nan, "FWHM_err": np.nan})
            else:
                fwhm, e_fwhm = v.split("+/-")
                formatted_headers.update({"FWHM": float(com), "FWHM_err": float(e_com)})
        else:  # other crap, keep as is
            formatted_headers.update({k: v})

    return formatted_headers


def _nenux_entry(nxentry, das_logs):
    """Entry containing scan info"""

    nxentry.attrs["NX_class"] = "NXentry"
    # nxentry.attrs["NX_class"] = "NXroot"
    nxentry.attrs["EX_required"] = "true"

    # Valid enumeration values for root['/entry']['definition'] are: NXtas
    nxentry.create_dataset(name="definition", data="NXtas", maxshape=None)
    nxentry["definition"].attrs["type"] = "NX_CHAR"
    nxentry["definition"].attrs["EX_required"] = "true"

    try:
        nxentry.create_dataset(name="title", data=das_logs.attrs["scan_title"], maxshape=None)
        nxentry["title"].attrs["type"] = "NX_CHAR"
        nxentry["title"].attrs["EX_required"] = "true"

        # TODO timezone
        start_date_time = "{} {}".format(das_logs.attrs["date"], das_logs.attrs["time"])
        start_time = datetime.strptime(start_date_time, "%m/%d/%Y %I:%M:%S %p").isoformat()

        nxentry.create_dataset(name="start_time", data=start_time, maxshape=None)
        nxentry["start_time"].attrs["type"] = "NX_DATE_TIME"
        nxentry["start_time"].attrs["EX_required"] = "true"

        # if "end_time" in das_logs.attrs:  # last scan never finished
        end_date_time = das_logs.attrs["end_time"]
        end_time = datetime.strptime(end_date_time, "%m/%d/%Y %I:%M:%S %p").isoformat()
        nxentry.create_dataset(name="end_time", data=end_time, maxshape=None)
        nxentry["end_time"].attrs["type"] = "NX_DATE_TIME"

    except KeyError:
        pass

    return nxentry


def _nexus_source(nxsource, das_logs, instrument_config_params=None):
    """Source"""

    nxsource.attrs["NX_class"] = "NXsource"
    nxsource.attrs["EX_required"] = "true"

    nxsource.create_dataset(name="name", data="HFIR", maxshape=None)
    nxsource["name"].attrs["type"] = "NX_CHAR"
    nxsource["name"].attrs["EX_required"] = "true"

    # Valid enumeration values for root['/entry/instrument/source']['probe'] are:
    # neutron of x-ray

    nxsource.create_dataset(name="probe", data="neutron", maxshape=None)
    nxsource["probe"].attrs["type"] = "NX_CHAR"
    nxsource["probe"].attrs["EX_required"] = "true"

    # Effective distance from sample Distance as seen by radiation from sample.
    # This number should be negative to signify that it is upstream of the sample.
    # nxsource.attrs["distance"] = -0.0

    return nxsource


def _nexus_mono(nxmono, das_logs, instrument_config_params=None):
    """Monochromator"""

    nxmono.attrs["NX_class"] = "NXcrystal"
    nxmono.attrs["EX_required"] = "true"

    try:
        nxmono.create_dataset(name="ei", data=das_logs["ei"], maxshape=None)
        nxmono["ei"].attrs["type"] = "NX_FLOAT"
        nxmono["ei"].attrs["EX_required"] = "true"
        # nxmono["ei"].attrs["axis"] = "1"
        nxmono["ei"].attrs["units"] = "meV"

        nxmono.create_dataset(name="type", data=das_logs.attrs["monochromator"], maxshape=None)
        nxmono.attrs["type"] = "NX_CHAR"

        nxmono.create_dataset(name="m1", data=das_logs["m1"], maxshape=None)
        nxmono["m1"].attrs["type"] = "NX_FLOAT"
        nxmono["m1"].attrs["units"] = "degrees"

        nxmono.create_dataset(name="m2", data=das_logs["m2"], maxshape=None)
        nxmono["m2"].attrs["type"] = "NX_FLOAT"
        nxmono["m2"].attrs["units"] = "degrees"

        nxmono.create_dataset(name="mfocus", data=das_logs["mfocus"], maxshape=None)
        nxmono["mfocus"].attrs["type"] = "NX_FLOAT"

        nxmono.create_dataset(name="marc", data=das_logs["marc"], maxshape=None)
        nxmono["marc"].attrs["type"] = "NX_FLOAT"

        nxmono.create_dataset(name="mtrans", data=das_logs["mtrans"], maxshape=None)
        nxmono["mtrans"].attrs["type"] = "NX_FLOAT"

        nxmono.create_dataset(name="focal_length", data=das_logs["focal_length"], maxshape=None)
        nxmono["focal_length"].attrs["type"] = "NX_FLOAT"

        nxmono.create_dataset(name="sense", data=das_logs.attrs["sense"][0], maxshape=None)
        nxmono.attrs["type"] = "NX_CHAR"

        # nxmono.create_dataset(name="rotation_angle", data=1.0, maxshape=None)
        # nxmono["rotation_angle"].attrs["type"] = "NX_FLOAT"
        # nxmono["rotation_angle"].attrs["EX_required"] = "true"
        # nxmono["rotation_angle"].attrs["units"] = "NX_ANGLE"

    except KeyError:
        pass

    return nxmono


def _nexus_ana(nxana, das_logs, instrument_config_params=None):
    """Analyzer"""
    nxana.attrs["NX_class"] = "NXcrystal"
    nxana.attrs["EX_required"] = "true"

    try:
        nxana.create_dataset(name="ef", data=das_logs["ef"], maxshape=None)
        nxana["ef"].attrs["type"] = "NX_FLOAT"
        nxana["ef"].attrs["EX_required"] = "true"
        # nxana["ef"].attrs["axis"] = "1"
        nxana["ef"].attrs["units"] = "meV"

        nxana.create_dataset(name="type", data=das_logs.attrs["analyzer"], maxshape=None)
        nxana.attrs["type"] = "NX_CHAR"

        nxana.create_dataset(name="a1", data=das_logs["a1"], maxshape=None)
        nxana["a1"].attrs["type"] = "NX_FLOAT"
        nxana["a1"].attrs["units"] = "degrees"

        nxana.create_dataset(name="a2", data=das_logs["a2"], maxshape=None)
        nxana["a2"].attrs["type"] = "NX_FLOAT"
        nxana["a2"].attrs["units"] = "degrees"

        nxana.create_dataset(name="afocus", data=das_logs["afocus"], maxshape=None)
        nxana["afocus"].attrs["type"] = "NX_FLOAT"

        for i in range(8):  # qm1--qm8, xm1 -- xm8
            nxana.create_dataset(name=f"qm{i+1}", data=das_logs[f"qm{i+1}"], maxshape=None)
            nxana.create_dataset(name=f"xm{i+1}", data=das_logs[f"xm{i+1}"], maxshape=None)

        nxana.create_dataset(name="sense", data=das_logs.attrs["sense"][2], maxshape=None)
        nxana.attrs["type"] = "NX_CHAR"

        # nxana.create_dataset(name="rotation_angle", data=1.0, maxshape=None)
        # nxana["rotation_angle"].attrs["type"] = "NX_FLOAT"
        # nxana["rotation_angle"].attrs["EX_required"] = "true"
        # nxana["rotation_angle"].attrs["units"] = "NX_ANGLE"

        # nxana.create_dataset(name="polar_angle", data=1.0, maxshape=None)
        # nxana["polar_angle"].attrs["type"] = "NX_FLOAT"
        # nxana["polar_angle"].attrs["EX_required"] = "true"
        # nxana["polar_angle"].attrs["units"] = "NX_ANGLE"

    except KeyError:
        pass
    return nxana


def _nexus_detector(nxdetector, das_logs, instrument_config_params=None):
    """Detector"""
    nxdetector.attrs["NX_class"] = "NXdetector"
    nxdetector.attrs["EX_required"] = "true"

    try:
        nxdetector.create_dataset(name="data", data=das_logs["detector"], maxshape=None, dtype="int")
        nxdetector["data"].attrs["type"] = "NX_INT"
        nxdetector["data"].attrs["EX_required"] = "true"
        nxdetector["data"].attrs["units"] = "counts"

        # nxdetector.create_dataset(name="polar_angle", data=1.0, maxshape=None)
        # nxdetector["polar_angle"].attrs["type"] = "NX_FLOAT"
        # nxdetector["polar_angle"].attrs["EX_required"] = "true"
        # nxdetector["polar_angle"].attrs["units"] = "NX_ANGLE"

    except KeyError:
        pass
    return nxdetector


def _nexus_monitor(nxmonitor, das_logs, instrument_config_params=None):
    """Monitor"""
    nxmonitor.attrs["NX_class"] = "NXmonitor"
    nxmonitor.attrs["EX_required"] = "true"

    try:  # --------------------------- monitor ---------------------------
        # Valid enumeration values for root['/entry/monitor']['mode'] are:monitor/time/ mcu
        if das_logs.attrs["preset_type"] == "normal":
            preset_channel = das_logs.attrs["preset_channel"]

            nxmonitor.create_dataset(name="mode", data=preset_channel, maxshape=None)
            nxmonitor["mode"].attrs["type"] = "NX_CHAR"
            nxmonitor["mode"].attrs["EX_required"] = "true"

            nxmonitor.create_dataset(name="preset", data=das_logs.attrs["preset_value"], maxshape=None)
            nxmonitor["preset"].attrs["type"] = "NX_FLOAT"
            nxmonitor["preset"].attrs["EX_required"] = "true"

            nxmonitor.create_dataset(name="time", data=das_logs["time"], maxshape=None)
            nxmonitor["time"].attrs["type"] = "NX_FLOAT"
            nxmonitor["time"].attrs["units"] = "seconds"

            nxmonitor.create_dataset(name="monitor", data=das_logs["monitor"], maxshape=None)
            nxmonitor["monitor"].attrs["type"] = "NX_INT"
            nxmonitor["monitor"].attrs["units"] = "counts"

            nxmonitor.create_dataset(name="mcu", data=das_logs["mcu"], maxshape=None)
            nxmonitor["mcu"].attrs["type"] = "NX_FLOAT"

            nxmonitor.create_dataset(name="data", data=das_logs[preset_channel], maxshape=None)
            nxmonitor["data"].attrs["type"] = "NX_FLOAT"
            nxmonitor["data"].attrs["EX_required"] = "true"
            # nxmonitor["data"].attrs["units"] = "counts"

        # TODO polarized exp at HB1
        elif das_logs.attrs["preset_type"] == "countfile":
            print("Polarization data, not yet supported.")

        else:
            print("Unrecogonized preset type. ")
    except KeyError:
        pass

    return nxmonitor


# TODO all sample environment variable names needed!
def _nexus_sample_environment(nxsample, das_logs):
    """Sample environment, e.g. cryostat, magnet"""
    temperatue_str = (
        ("temp", "temp_a", "temp_2", "coldtip", "tsample", "sample")
        + ("vti", "dr_tsample", "dr_temp")
        + ("lt", "ht", "sorb_temp", "sorb", "sample_ht")
    )

    field_str = ("persistent_field", "mag_i")

    try:
        for t in temperatue_str:
            nxsample.create_dataset(name=t, data=das_logs[t], maxshape=None)
            nxsample["" + t].attrs["type"] = "NX_FLOAT"
            nxsample["" + t].attrs["units"] = "K"

        for f in field_str:
            nxsample.create_dataset(name=f, data=das_logs[f], maxshape=None)
            nxsample["" + t].attrs["type"] = "NX_FLOAT"
            nxsample["" + t].attrs["units"] = "T"

    except KeyError:
        pass

    return nxsample


def _nexus_sample(nxsample, das_logs, sample_config_params=None):
    "Sample and sample environment"
    nxsample.attrs["NX_class"] = "NXsample"
    nxsample.attrs["EX_required"] = "true"

    try:

        nxsample.create_dataset(name="name", data=das_logs.attrs["samplename"], maxshape=None)
        nxsample["name"].attrs["type"] = "NX_CHAR"
        nxsample["name"].attrs["EX_required"] = "true"

        nxsample.create_dataset(name="qh", data=das_logs["h"], maxshape=None)
        nxsample["qh"].attrs["type"] = "NX_FLOAT"
        nxsample["qh"].attrs["EX_required"] = "true"
        # nxsample["qh"].attrs["axis"] = "1"
        # nxsample["qh"].attrs["units"] = "NX_DIMENSIONLESS"

        nxsample.create_dataset(name="qk", data=das_logs["k"], maxshape=None)
        nxsample["qk"].attrs["type"] = "NX_FLOAT"
        nxsample["qk"].attrs["EX_required"] = "true"
        # nxsample["qk"].attrs["axis"] = "1"
        # nxsample["qk"].attrs["units"] = "NX_DIMENSIONLESS"

        nxsample.create_dataset(name="ql", data=das_logs["l"], maxshape=None)
        nxsample["ql"].attrs["type"] = "NX_FLOAT"
        nxsample["ql"].attrs["EX_required"] = "true"
        # nxsample["ql"].attrs["axis"] = "1"
        # nxsample["ql"].attrs["units"] = "NX_DIMENSIONLESS"

        nxsample.create_dataset(name="en", data=das_logs["e"], maxshape=None)
        nxsample["en"].attrs["type"] = "NX_FLOAT"
        nxsample["en"].attrs["EX_required"] = "true"
        # nxsample["en"].attrs["axis"] = "1"
        nxsample["en"].attrs["units"] = "meV"

        nxsample.create_dataset(name="sgu", data=das_logs["sgu"], maxshape=None)
        nxsample["sgu"].attrs["type"] = "NX_FLOAT"
        nxsample["sgu"].attrs["EX_required"] = "true"
        nxsample["sgu"].attrs["units"] = "degrees"

        nxsample.create_dataset(name="sgl", data=das_logs["sgl"], maxshape=None)
        nxsample["sgl"].attrs["type"] = "NX_FLOAT"
        nxsample["sgl"].attrs["EX_required"] = "true"
        nxsample["sgl"].attrs["units"] = "degrees"

        nxsample.create_dataset(name="unit_cell", data=das_logs.attrs["latticeconstants"], maxshape=None)
        nxsample["unit_cell"].attrs["type"] = "NX_FLOAT"
        nxsample["unit_cell"].attrs["EX_required"] = "true"
        # nxsample["unit_cell"].attrs["units"] = "NX_LENGTH"

        nxsample.create_dataset(name="orientation_matrix", data=das_logs.attrs["ubmatrix"], maxshape=None)
        nxsample["orientation_matrix"].attrs["type"] = "NX_FLOAT"
        nxsample["orientation_matrix"].attrs["EX_required"] = "true"
        nxsample["orientation_matrix"].attrs["units"] = "NX_DIMENSIONLESS"

        nxsample.create_dataset(name="ub_conf", data=das_logs.attrs["ubconf"].split(".")[0], maxshape=None)
        nxsample["ub_conf"].attrs["type"] = "NX_CHAR"

        nxsample.create_dataset(name="plane_normal", data=das_logs.attrs["plane_normal"], maxshape=None)
        nxsample["plane_normal"].attrs["type"] = "NX_FLOAT"

        nxsample.create_dataset(name="q", data=das_logs["q"], maxshape=None)
        nxsample["q"].attrs["type"] = "NX_FLOAT"
        nxsample["q"].attrs["units"] = "Angstrom^-1"

        nxsample.create_dataset(name="stu", data=das_logs["stu"], maxshape=None)
        nxsample["stu"].attrs["type"] = "NX_FLOAT"
        nxsample["stu"].attrs["units"] = "degrees"

        nxsample.create_dataset(name="stl", data=das_logs["stl"], maxshape=None)
        nxsample["stl"].attrs["type"] = "NX_FLOAT"
        nxsample["stl"].attrs["units"] = "degrees"

        nxsample.create_dataset(name="s1", data=das_logs["s1"], maxshape=None)
        nxsample["s1"].attrs["type"] = "NX_FLOAT"
        nxsample["s1"].attrs["units"] = "degrees"

        nxsample.create_dataset(name="s2", data=das_logs["s2"], maxshape=None)
        nxsample["s2"].attrs["type"] = "NX_FLOAT"
        nxsample["s2"].attrs["units"] = "degrees"

        nxsample.create_dataset(name="type", data=das_logs.attrs["sampletype"], maxshape=None)
        nxsample["type"].attrs["type"] = "NX_CHAR"

        nxsample.create_dataset(name="sense", data=das_logs.attrs["sense"][1], maxshape=None)
        nxsample.attrs["type"] = "NX_CHAR"

        nxsample.create_dataset(name="Pt.", data=das_logs["Pt."], maxshape=None)
        nxsample.attrs["type"] = "NX_CHAR"

        # nxsample.create_dataset(name="rotation_angle", data=1.0, maxshape=None)
        # nxsample["rotation_angle"].attrs["type"] = "NX_FLOAT"
        # nxsample["rotation_angle"].attrs["EX_required"] = "true"
        # nxsample["rotation_angle"].attrs["units"] = "NX_ANGLE"

        # nxsample.create_dataset(name="polar_angle", data=1.0, maxshape=None)
        # nxsample["polar_angle"].attrs["type"] = "NX_FLOAT"
        # nxsample["polar_angle"].attrs["EX_required"] = "true"
        # nxsample["polar_angle"].attrs["units"] = "NX_ANGLE"
    except KeyError:
        pass

    nxsample = _nexus_sample_environment(nxsample, das_logs)

    return nxsample


# TODO div_y
def _nexus_coll(nxcoll, das_logs, instrument_config_params=None):
    """Collimators"""
    nxcoll.attrs["NX_class"] = "NXcollimator"

    try:
        nxcoll.create_dataset(name="type", data="Soller", maxshape=None)
        nxcoll["type"].attrs["type"] = "NX_CHAR"

        div_x = [float(v) for v in list(das_logs.attrs["collimation"].split("-"))]
        nxcoll.create_dataset(name="divergence_x", data=div_x, maxshape=None)
        nxcoll["divergence_x"].attrs["type"] = "NX_ANGLE"
        nxcoll["divergence_x"].attrs["units"] = "minutes of aarc"

    except KeyError:
        pass
    return nxcoll


def _nexus_slits(nxslit, das_logs, instrument_config_params=None):
    """motorized slits"""

    nxslit.attrs["NX_class"] = "NXslit"

    slits_str1 = tuple([f"b{idx}{loc}" for idx in ("a", "b") for loc in ("t", "b", "l", "r")])
    slits_str2 = tuple([f"slit{idx}_{loc}" for idx in ("a", "b") for loc in ("lf", "rt", "tp", "bt")])
    slits_str3 = tuple([f"slit_{idx}_{loc}" for idx in ("pre",) for loc in ("lf", "rt", "tp", "bt")])

    slits_str = (slits_str1, slits_str2, slits_str3)

    for slit_str in slits_str:
        if slit_str[0] in das_logs.keys():
            for st in slit_str:
                nxslit.create_dataset(name=st, data=das_logs[st])
                nxslit[st].attrs["type"] = "NX_FLOAT"
                nxslit[st].attrs["units"] = "cm"

    return nxslit


# TODO HB1 polarized experiment, Helmohtz coils guide fields: tbguide, aguide, bguide
def _nexus_flipper(nxflipper, das_logs, instrument_config_params=None):
    nxflipper.attrs["NX_class"] = "NXflipper"

    try:
        nxflipper.create_dataset(name="fguide", data=das_logs["fguide"], maxshape=None)
        nxflipper["fguide"].attrs["type"] = "NX_FLOAT"

        nxflipper.create_dataset(name="hguide", data=das_logs["hguide"], maxshape=None)
        nxflipper["hguide"].attrs["type"] = "NX_FLOAT"

        nxflipper.create_dataset(name="vguide", data=das_logs["vguide"], maxshape=None)
        nxflipper["vguide"].attrs["type"] = "NX_FLOAT"

    except KeyError:
        pass

    return nxflipper


def _nexus_data(nxentry, das_logs):
    """create links to data"""
    nxdata = nxentry.create_group("data")
    nxdata.attrs["NX_class"] = "NXdata"
    nxdata.attrs["EX_required"] = "true"

    nexus_keywork_conversion_dict = {"h": "qh", "k": "qk", "l": "ql", "e": "en"}
    def_x = das_logs.attrs["def_x"]
    def_y = das_logs.attrs["def_y"]
    if def_x in nexus_keywork_conversion_dict:
        def_x = nexus_keywork_conversion_dict[def_x]

    path_x = _find_val(def_x, nxentry)
    path_y = _find_val(def_y, nxentry)
    if def_y == "detector" or def_y == "monitor":
        path_y += "/data"

    if path_y is not None:
        if nxentry.get(path_y[1:]) is not None:
            # Create the LINKS
            nxentry["data/" + def_y] = h5py.SoftLink(nxentry.name + path_y)
            nxentry["data/" + def_y + "/"].attrs["target"] = nxentry.name + path_y
            # if def_y == "detector" or def_y == "monitor":
            #     nxentry["data"].attrs["signal"] = "data"
            # else:
            nxentry["data"].attrs["signal"] = def_y
    if path_x is not None:
        if nxentry.get(path_x[1:]) is not None:
            nxentry["data/" + def_x] = h5py.SoftLink(nxentry.name + path_x)
            nxentry["data/" + def_x + "/"].attrs["target"] = nxentry.name + path_x

            if def_x in nexus_keywork_conversion_dict:
                nxentry["data"].attrs["axes"] = nexus_keywork_conversion_dict[def_x]

            else:
                nxentry["data"].attrs["axes"] = def_x
    return nxentry


def _daslogs_to_nexus(
    nxentry,
    das_str: Literal["SPICElogs", "DASlogs"] = "SPICElogs",
    instrument_config_params: Optional[dict] = None,
    sample_config_params: Optional[dict] = None,
):
    """Format info from SPICElogs into Nexus format"""

    das_logs = nxentry[das_str]

    nxentry = _nenux_entry(nxentry, das_logs)

    nxentry.create_group("instrument")
    nxentry["instrument"].attrs["NX_class"] = "NXinstrument"
    nxentry["instrument"].attrs["EX_required"] = "true"
    nxentry["instrument"].create_dataset(name="name", data=das_logs.attrs["instrument"], maxshape=None)
    nxentry["instrument/name"].attrs["type"] = "NX_CHAR"

    nxsource = nxentry["instrument/"].create_group("source")
    nxsource = _nexus_source(nxsource, das_logs, instrument_config_params)

    nxcoll = nxentry["instrument/"].create_group("collimator")
    nxcoll = _nexus_coll(nxcoll, das_logs, instrument_config_params)

    nxmono = nxentry["instrument/"].create_group("monochromator")
    nxmono = _nexus_mono(nxmono, das_logs, instrument_config_params)

    nxana = nxentry["instrument/"].create_group("analyser")
    nxana = _nexus_ana(nxana, das_logs, instrument_config_params)

    nxdetector = nxentry["instrument/"].create_group("detector")
    nxdetector = _nexus_detector(nxdetector, das_logs, instrument_config_params)

    nxmonitor = nxentry.create_group("monitor")
    nxmonitor = _nexus_monitor(nxmonitor, das_logs, instrument_config_params)

    nxsample = nxentry.create_group("sample")
    nxsample = _nexus_sample(nxsample, das_logs, sample_config_params)

    nxslit = nxentry["instrument/"].create_group("slit")
    nxslit = _nexus_slits(nxslit, das_logs, instrument_config_params)

    nxflipper = nxentry["instrument/"].create_group("flipper")
    nxflipper = _nexus_flipper(nxflipper, das_logs, instrument_config_params)

    # nxentry["instrument"].create_group("filter")
    # nxentry["instrument"].attrs["NX_class"] = "NXfilter"

    nxentry = _nexus_data(nxentry, das_logs)

    # Create the DOC strings
    # nxentry["definition"].attrs["EX_doc"] = "Official NeXus NXDL schema to which this file conforms "
    # nxsample["name"].attrs["EX_doc"] = "Descriptive name of sample "
    # nxmonitor["mode"].attrs[ "EX_doc"] = \
    # "Count to a preset value based on either clock time (timer) or received monitor counts (monitor). "
    # nxmonitor["preset"].attrs["EX_doc"] = "preset value for time or monitor "
    # nxmonitor["data"].attrs["EX_doc"] = "Total integral monitor counts "
    # nxentry["data"].attrs["EX_doc"] = "One of the ei,ef,qh,qk,ql,en should get a primary=1 "+\
    # "attribute to denote the main scan axis "

    return nxentry


def convert_spice_ub_to_nexus(
    path_to_spice_folder: str,
    path_to_hdf5_folder: str,
    verbose: bool = False,
) -> None:
    """Convert all UBConf files into one NeXus entry named UBConf.h5"""

    if verbose:
        disp_str = (
            f"Converting SPICE UBConf files at {path_to_spice_folder} to an NeXus entry at {path_to_hdf5_folder}"
        )
        print(disp_str)

    p = Path(path_to_spice_folder)
    ub_files = sorted((p / "UBConf").glob("*.ini"))
    tmp_ub_files = sorted((p / "UBConf/tmp").glob("*.ini"))
    ub_files_all = ub_files + tmp_ub_files
    ub_conf_dicts = {ub_file.parts[-1].split(".")[0]: _read_spice_ub(ub_file) for ub_file in ub_files_all}

    with h5py.File(path_to_hdf5_folder + "UBConf.h5", "w") as root:

        for ub_name, ub_data in ub_conf_dicts.items():
            ub_entry = root.create_group(ub_name)
            ub_entry.attrs["NX_class"] = "NXcollection"
            ub_entry.attrs["X_required"] = "false"
            for k, v in ub_data.items():
                ub_entry.create_dataset(name=k, data=v, maxshape=None)


# TODO implement parameter loading from json
def convert_spice_scan_to_nexus(
    path_to_scan_file: str,
    path_to_hdf5_folder: str,
    path_to_instrument_json: Optional[str] = None,
    path_to_sample_json: Optional[str] = None,
    verbose: bool = False,
) -> None:
    """Convert a SPICE scan file at path_to_scan_file to an NeXus entry in the folder of path_to_hdf5_folder

    Note:
        path_to_instrument_json and path_to_sample_json are needed to provide extra information
        for resolution calculation.
    """

    (
        spice_data,
        col_headers,
        headers,
        unused,
        error_messages,
    ) = _read_spice(path_to_scan_file)
    ipts = headers["proposal"]
    spice_file_name = path_to_scan_file.split("/")[-1]  # e.g. CG4C_exp0424_scan0001.dat
    instrument_str, exp_num, scan_num = spice_file_name.split("_")
    scan_num = scan_num.split(".")[0]  # e.g. "scan0001"
    scan_name = path_to_hdf5_folder + scan_num + ".h5"
    formatted_headers = _format_spice_header(headers)

    if verbose:
        disp_str = f"Converting SPICE data {spice_file_name}"
        print(disp_str)

    # parse instruemnt and sample json files
    if path_to_instrument_json is not None:
        instrument_config = Path(path_to_instrument_json)
        if instrument_config.is_file():
            with open(instrument_config, "r", encoding="utf-8") as file:
                instrument_config_params = json.load(file)
    else:
        instrument_config_params = None

    if path_to_sample_json is not None:
        sample_config = Path(path_to_sample_json)
        if sample_config.is_file():
            with open(sample_config, "r", encoding="utf-8") as file:
                sample_config_params = json.load(file)
    else:
        sample_config_params = None

    with h5py.File(scan_name, "w") as root:
        dataset_name = f"IPTS{ipts}_{instrument_str}_{exp_num}"
        nxentry = root.create_group(scan_num)
        nxentry.attrs["dataset_name"] = dataset_name
        # create SPICElogs
        spice_logs = nxentry.create_group("SPICElogs")
        spice_logs.attrs["NX_class"] = "NXcollection"
        spice_logs.attrs["X_required"] = "false"
        spice_logs.attrs["instrument"] = instrument_str

        # write SPICElogs attributes
        for k, v in formatted_headers.items():
            spice_logs.attrs[k] = v
        if len(error_messages) != 0:
            spice_logs.attrs["Error Messages"] = error_messages

        # write SPICElogs datasets
        spice_data_shape = spice_data.shape

        if len(spice_data_shape) == 1:  # 1 row ony
            if spice_data_shape[0] != 0:
                for idx, col_header in enumerate(col_headers):
                    spice_logs.create_dataset(col_header, data=spice_data[idx])
            else:  # ignore if empty
                pass
        elif len(spice_data_shape) == 2:  # nomarl data with mutiple rows
            # print(scan_num)
            # print(spice_data.shape)
            for idx, col_header in enumerate(col_headers):
                spice_logs.create_dataset(col_header, data=spice_data[:, idx])

        # Format SPICElogs to NeXus
        nxentry = _daslogs_to_nexus(
            nxentry,
            das_str="SPICElogs",
            instrument_config_params=instrument_config_params,
            sample_config_params=sample_config_params,
        )

        # Create the ATTRIBUTES
        nxentry.attrs["file_name"] = os.path.abspath(scan_name)
        nxentry.attrs["file_time"] = datetime.now().isoformat()
        nxentry.attrs["h5py_version"] = h5py.version.version
        nxentry.attrs["HDF5_Version"] = h5py.version.hdf5_version


def convert_spice_to_nexus(
    path_to_spice_folder: str,
    path_to_hdf5_folder: Optional[str] = None,
    path_to_instrument_json: Optional[str] = None,
    path_to_sample_json: Optional[str] = None,
    verbose: bool = False,
) -> None:
    """Load data from spice folder. Convert to a nexus file

    Args:
        path_to_spice_folder (str): spice folder, e.g. ./exp424/
        path_to_nexus (str): path to hdf5 folder, e.g. ./exp424_nexus/
        path_to_instrument_json (str): path to a json file containing instrument
        configuration
        path_to_sample_json (str): path to a json file containing instrument
        configuration

    Note:
        if path_to_nexus is not given, create a folder in the same
        parent folder as path_to_spice_folder, named expXXX_nexus
    """
    if path_to_spice_folder[-1] != "/":
        path_to_spice_folder += "/"

    scan_list = os.listdir(path_to_spice_folder + "Datafiles/")
    scan_list = [path_to_spice_folder + "Datafiles/" + scan for scan in scan_list if scan.endswith(".dat")]
    scan_list.sort()

    # get IPTS number and instrument string
    first_scan = scan_list[0]
    (_, _, headers, _, _) = _read_spice(first_scan)
    ipts = headers["proposal"]
    spice_file_name = first_scan.split("/")[-1]  # e.g. CG4C_exp0424_scan0001.dat
    instrument_str, exp_num, _ = spice_file_name.split("_")

    if path_to_hdf5_folder is None:
        parent_path = "/".join(path_to_spice_folder.split("/")[:-2])
        path_to_hdf5_folder = parent_path + f"/IPTS{ipts}_{instrument_str}_{exp_num}/"
    elif path_to_hdf5_folder[-1] != "/":
        path_to_hdf5_folder += "/"
    if not os.path.exists(path_to_hdf5_folder):
        os.makedirs(path_to_hdf5_folder)

    if verbose:
        print(f"Converting SPICE data at {path_to_spice_folder} to NeXus at {path_to_hdf5_folder}")

    # p = Path(path_to_spice_folder)
    # scans = sorted((p / "Datafiles").glob("*.dat"))

    for path_to_scan_file in scan_list:
        convert_spice_scan_to_nexus(
            path_to_scan_file,
            path_to_hdf5_folder,
            path_to_instrument_json,
            path_to_sample_json,
            verbose,
        )

    convert_spice_ub_to_nexus(path_to_spice_folder, path_to_hdf5_folder, verbose)
