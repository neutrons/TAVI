# -*- coding: utf-8 -*-
import os
from typing import Any

import numpy as np


def read_spice_datafile(file_name: str):
    """Reads an ascii generated by spice, and returns a header structure and a data table

    Args:
        file_name (str): a string containing the filename

    Returns:
        data (np.ndarray): an array containing all columns/rows
        metadata (dict): a dictionary containing information from the commented lines.
        col_names (tuple): name of each collum in spice_data
        others (tuple): unrecogonized lines
        error_messages(tuple): contains line of error message such as unreachable motor positions
    """
    with open(file_name, encoding="utf-8") as f:
        all_content = f.readlines()

    headers = [line.strip() for line in all_content if "#" in line]
    index_col_name = headers.index("# col_headers =")
    col_names = headers[index_col_name + 1].strip("#").split()
    # remove the dot before it causes problem
    # index_of_pt = col_names.index("Pt.")
    # col_names[index_of_pt] = "Pt"
    metadata_list = headers[:index_col_name]
    error_messages = headers[index_col_name + 2 :]

    index_sum_count = [i for i, header in enumerate(headers) if header.startswith("# Sum of Counts =")]
    # in case "Sum of Counts" doesn't exist
    # happens to the last scan after beam is down
    if len(index_sum_count) != 0:
        metadata_list += headers[index_sum_count[0] :]
        error_messages = error_messages[: index_sum_count[0] - len(headers)]

    metadata = {}
    others = []

    for metadata_entry in metadata_list:
        line = metadata_entry.strip("# ")

        if "completed" in line or "stopped" in line:  # last line
            parts = line.split(" ")
            end_time = parts[3] + " " + parts[0] + " " + parts[1]
            metadata.update({"end_time": end_time})
        # elif line[-1] == "=":  # empty line
        #     unused.append(line[:-2])  # remove  " ="
        elif "=" in line:  # useful line
            parts = line.split("=")
            key = parts[0].strip()
            val = "=".join(parts[1:])[1:]  # remove the fisrt space charactor
            metadata.update({key: val})
        else:  # how did you get here?
            others.append(line)
    # others = tuple(others)

    data = np.genfromtxt(file_name, comments="#")

    return (data, tuple(col_names), metadata, tuple(others), tuple(error_messages))


def read_spice_ubconf(ub_file_name: str) -> dict:
    """Reads ub info from UBConf
    Args:
        ub_file_name (str): a string containing the filename

    Returns:
    """
    ubconf: dict[str, Any] = {}
    with open(ub_file_name, encoding="utf-8") as f:
        all_content = f.readlines()

    for idx, line in enumerate(all_content):
        if line.strip() == "":
            continue  # skip if empty
        elif line.strip()[0] == "[":
            continue  # skiplines like "[xx]"

        key, val = line.strip().split("=")

        if key == "Mode":
            mode_name = all_content[idx - 1].strip()
            if mode_name == "[UBMode]":
                ubconf.update({"UBMode": int(val)})
            elif mode_name == "[AngleMode]":
                ubconf.update({"AngleMode": int(val)})
        elif "," in val:  # string of vector to array
            ubconf.update({key: np.array([float(v) for v in val.strip('"').split(",")])})
        else:  # float
            ubconf.update({key: float(val)})

    return ubconf


def _create_spicelogs(path_to_scan_file: str) -> dict:
    """read in SPICE data, return a dictionary containing metadata and data columns"""

    (data, col_names, metadata, others, error_messages) = read_spice_datafile(path_to_scan_file)
    *_, file_name = path_to_scan_file.split("/")
    instrument_str, *_ = file_name.split("_")
    # write SPICElogs attributes
    attrs_dict = {"instrument": instrument_str}
    for k, v in metadata.items():
        attrs_dict.update({k: v})
    if len(error_messages) != 0:
        attrs_dict.update({"Error Messages": error_messages})
    if len(others) != 0:
        attrs_dict.update({"Others": others})

    # write SPICElogs datasets
    dataset_dict = {}
    spice_data_shape = data.shape
    if len(spice_data_shape) == 1:  # 1 row ony
        if spice_data_shape[0] != 0:
            for idx, col_header in enumerate(col_names):
                # convert to ndarray if one point only
                dataset_dict.update({col_header: np.array([data[idx]])})
        else:  # ignore if empty
            pass
    elif len(spice_data_shape) == 2:  # nomarl data with mutiple rows
        # print(scan_num)
        # print(spice_data.shape)
        for idx, col_header in enumerate(col_names):
            dataset_dict.update({col_header: data[:, idx]})

    scan_path = os.path.abspath(path_to_scan_file)
    (*folder_path, _, _) = scan_path.split("/")
    ub_file_path = "/".join(folder_path + ["UBConf", metadata["ubconf"]])
    ub_conf_dict = {"file_path": ub_file_path}
    ubconf = read_spice_ubconf(ub_file_path)
    for k, v in ubconf.items():
        ub_conf_dict.update({k: v})

    spicelogs = {"metadata": attrs_dict, "ub_conf": ub_conf_dict}
    spicelogs.update(dataset_dict)
    return spicelogs
